
\chapter{Multiple regression}
\label{sec:multiple}

Bivariate regression fits a line to a two-dimensional space. If we have a regression with two predictors we fit a plane to a three-dimensional space. For example,
\begin{equation}
\hat{y}_i=\beta_0+\beta_1x_i+\beta_2z_i
\end{equation}
is an equation for plane and provides a formula for points on a (y,x,z) coordinate system. By fitting a plane to the data, regression can independently estimate the effects of one variable, while holding the other constant. The best way to think about multiple regression is with matrix algebra

\section{The matrix algebra formulation}
Thus, one of the beauties of regression is that we can have more than one predictor. Using two variables for example, we could fit a plane to the variables $x_1$ and $x_2$:
\begin{equation}
\hat{y}_i=\beta_0+\beta_1x_{1i}+\beta_2x_{2i}
\end{equation}
We could fit a hyperplane to three predictors, indexed as $x_1$, $x_2$, and $x_3$:
\begin{equation}
\hat{y}_i=\beta_0+\beta_1x_{1i}+\beta_2x_{2i}+\beta_3x_{3i}
\end{equation}
and so on.
The problem is the algebra for these solutions gets complicated fast. Instead, we have a matrix algebra equivalent of our least squares estimator
\begin{equation}\label{eq:olsmatrix}
{\bf{b=\left(X'X\right)^{-1}X'y}}
\end{equation}
\begin{table}[htbp]\centering
\caption{Small set of 4 cases\label{tab:mxy}
\textbf{} }\begin{tabular} {@{} l r @{}} \\
$y$ & $x$ \\
\hline
3 &4\\
2 &3\\
4 &3\\
5 &6\\
\hline
\multicolumn{2}{@{}l}{\footnotesize{\emph{} }}
\end{tabular}
\end{table}

Matrix algebra is powerful. It deals with vectors (single rows or columns of data) and matrixes (data with multiple rows and columns). Statistics packages take the data in tables and convert them to matrixes. For instance, we can think of our outcome in Table~\ref{tab:mxy} as a vector of numbers called ${\bf{y}}$, for example:
\[
{\bf{y}}=
\begin{bmatrix}
3 \\
2 \\
4 \\
5 \\
\end{bmatrix}
\]
Here we have 4 cases ($N$ = 4) where the first case of $y$ is 3, the second case of $y$ is 2, the third case of $y$ is 4, and the fourth case of $y$ is 5. We can then think of our predictors as a matrix with $N$ rows and $p$ columns
\[
{\bf{X}}=
\begin{bmatrix}
1&4 \\
1&3 \\
1&3 \\
1&6 \\
\end{bmatrix}
\]
The first column is for the intercept. That's why some programs call it a constant, because it is for a "variable" that is a constant value of 1s. Thus, we can then think of our typical regression equation as
\begin{equation}\label{eq:matrixolsmodel}
{\bf{y=Xb+e}}
\end{equation}
\[
\begin{bmatrix}
y_i \\
\vdots \\
y_N \\
\end{bmatrix}
= \\
\begin{bmatrix}
1&x_{11}&\cdots&x_{p11} \\
\vdots&\vdots&\ddots&\vdots \\
1&x_{1N}&\cdots&x_{pN} \\
\end{bmatrix}
\begin{bmatrix}
\beta_0 \\
\beta_1 \\
\vdots \\
\beta_p \\
\end{bmatrix}
+ \\
\begin{bmatrix}
e_1 \\
\vdots \\
e_N \\
\end{bmatrix}
\]
Note how easy it is to formulate any number of predictors, the regression equation always comes out to ${\bf{y=Xb+e}}$. Now, the formula to obtain the slopes (or the b vector) is
\[
{\bf{b=\left(X'X\right)^{-1}X'y}}
\]
This may not look familiar, but it is. For instance,
\[
{\bf{\left(X'X\right)}}=\sum_{i=1}^N\left(x_i-\bar{x}\right)^2
\]
which is the denominator of the slope formula, and since
\[
a^{-1} = \frac{1}{a}
\]
Also,
\[
{\bf{X'y}} =\sum_{i=1}^N\left(y_i-\bar{y}\right)\left(x_i-\bar{x}\right)
\]
which is the numerator of the slope formula.

Matrix algebra has many rules, and I'm not going to go through all of them here. However, here is an example calculation with the small set of 4 in Table~\ref{tab:mxy}. First, we need to transpose ${\bf X}$ to get ${\bf X'}$ (also known as ${\bf X}$ prime or the transpose of ${\bf X}$):
\[
{\bf X'} =
\begin{bmatrix}
1&1&1&1 \\
4&3&3&6 \\
\end{bmatrix}
\]
Note that the transpose of ${\bf X}$ simply makes rows out of columns. Column 1 becomes row 1, column 2 becomes row 2, and so on. ${\bf X'X}$ is multiplying ${\bf X'}$ and ${\bf X}$ (note: in matrix algebra the order of multiplication matter a lot). So, if
\[
{\bf X'} =
\begin{bmatrix}
a_{11}&a_{12}&a_{13}&a_{14} \\
a_{21}&a_{22}&a_{23}&a_{24} \\
\end{bmatrix}
\]
and
\[
{\bf X} =
\begin{bmatrix}
b_{11} & b_{12} \\
b_{21} & b_{22} \\
b_{31} & b_{32} \\
b_{41} & b_{42} \\
\end{bmatrix}
\]
then
\begin{equation}
{\bf X'X} =
\begin{bmatrix}
\left(\sum_ia_{1i}b_{i1}\right)&\left(\sum_ia_{1i}b_{i2}\right) \\
\left(\sum_ia_{2i}b_{i1}\right)&\left(\sum_ia_{2i}b_{i2}\right) \\
\end{bmatrix}
\end{equation}
This means that we can compute X'X like so
\[
\sum_ia_{1i}b_{i1} = \left(\left(1\times1\right)+(\left(1\times1\right)+(\left(1\times1\right)+(\left(1\times1\right)\right)
\]
\[
\sum_ia_{1i}b_{i2} = \left(\left(4\times1\right)+(\left(3\times1\right)+(\left(3\times1\right)+(\left(6\times1\right)\right)
\]
\[
\sum_ia_{2i}b_{i1} = \left(\left(1\times4\right)+(\left(1\times3\right)+(\left(1\times3\right)+(\left(1\times6\right)\right)
\]
\[
\sum_ia_{2i}b_{i2} = \left(\left(4\times4\right)+(\left(3\times3\right)+(\left(3\times3\right)+(\left(6\times6\right)\right)
\]
which means
\[
{\bf X'X} =
\begin{bmatrix}
4&16 \\
16&70 \\
\end{bmatrix}
\]
Next, we tackle ${\bf X'y}$. If
\[
{\bf y} =
\begin{bmatrix}
b_{11} \\
b_{12} \\
b_{13} \\
b_{14} \\
\end{bmatrix}
\]
then
\begin{equation}
{\bf X'y} =
\begin{bmatrix}
\left(\sum_ia_{1i}b_{i1}\right) \\
\left(\sum_ia_{2i}b_{i1}\right) \\
\end{bmatrix}
\end{equation}
which translates into
\[
\sum_ia_{1i}b_{i1} = \left(1\times3\right)+\left(1\times2\right)+\left(1\times4\right)+\left(1\times5\right)
\]
\[
\sum_ia_{2i}b_{i1} = \left(4\times3\right)+\left(3\times2\right)+\left(3\times4\right)+\left(6\times5\right)
\]
\[
{\bf X'y} =
\begin{bmatrix}
14 \\
16 \\
\end{bmatrix}
\]
Now, we need to invert $\bf X'X$. So, if
\[
{\bf X'X} =
\begin{bmatrix}
a&b \\
c&d \\
\end{bmatrix}
\]
and
\begin{equation}
\left({\bf X'X}\right)^{-1} =
\begin{bmatrix}
\left(\frac{d}{ad-bc}\right)&\left(\frac{-b}{ad-bc}\right) \\
\left(\frac{-c}{ad-bc}\right)&\left(\frac{a}{ad-bc}\right) \\
\end{bmatrix}
\end{equation}
and also if
\[
ad-bc = 24
\]
then
\[
\left({\bf X'X}\right)^{-1} =
\begin{bmatrix}
2.92&-0.67 \\
-0.67&0.16 \\
\end{bmatrix}
\]
Bring it all together:
\[
{\bf b = \left(X'X\right)^{-1}X'y} =
\begin{bmatrix}
\left(\sum_ia_{1i}b_{i1}\right) \\
\left(\sum_ia_{2i}b_{i1}\right) \\
\end{bmatrix}
\]
Which means we are left with
\[
\bf b =
\begin{bmatrix}
0.83 \\
0.66 \\
\end{bmatrix}
\]
where 0.83 is our intercept, $\beta_0$, and 0.66 is our slope $\beta_1$. There we are. It is not important that you be able to do this, but I do want you to remember the formula $\bf b = \left(X'X\right)^{-1}X'y$ because we are going to alter it from time to time for different estimators.

\subsection{The matrix formulation of least squares}

We can still show that these are least squares estimates. In matrix form, the sum of squares is $\bf e'e$, so we can write the matrix of slopes as a function of the sum of squares
\[
S\left({\bf b}\right)={\bf e'e}
\]
\[
S\left({\bf b}\right)={\bf \left(y-Xb\right)'\left(y-Xb\right)}
\]
\begin{equation}
S\left({\bf b}\right)={\bf y'y-\left(2y'X\right)b+b'\left(X'X'\right)b}
\end{equation}
We then again find the derivative
\begin{equation}
\frac{\partial S\left({\bf b}\right)}{\partial {\bf b}}={\bf 2X'Xb-2X'y}
\end{equation}
set the derivative to 0 and solve
\[
0={\bf 2X'Xb-2X'y}
\]
\[
{\bf -2X'Xb}={\bf -2X'y}
\]
\[
{\bf X'Xb}={\bf X'y}
\]
\[
{\bf b}={\bf \left(X'X\right)^{-1}X'y}
\]
the result matches equation~\eqref{eq:olsmatrix}.

\subsection{The matrix formulation of maximum likelihood}
\label{sec:regml}

We can also get to this estimate through maximum likelihood. If the mean is equal to the slopes and predictors combination, $\bf Xb$, then the likelihood of observing our data from our parameters is
\begin{equation}
\mbox{ln}\left(L\left({\bf b, V \vert \bf y}\right)\right)=-\frac{N}{2}\mbox{ln}\left(2\pi\right)-\frac{N}{2}\mbox{ln}\left(\bf V\right)-\frac{1}{2}{\bf \left(y-Xb\right))'V^{-1}\left(y-Xb\right)}
\end{equation}
 where $\bf V$ is the variance-covariance matrix of residuals. After some work, we can find the partial derivative of this function with respect to the slopes, $\bf b$ as
\begin{equation}
\frac{\partial\mbox{ln}\left(L\left({\bf b, V \vert \bf y}\right)\right)}{\partial \bf b} = {\bf -X'V^{-1}Xb+X'V^{-1}y}
\end{equation}
we then set to 0 and solve
\[
0= {\bf -X'V^{-1}Xb+X'V^{-1}y}
\]
\[
{\bf X'V^{-1}Xb}= {\bf X'V^{-1}y}
\]
\[
{\bf b}= {\bf \left(X'V^{-1}X\right)^{-1}X'V^{-1}y}
\]
As we see in Chapter~\ref{sec:het}, a very important set of assumptions in OLS regression is that the errors have 0 correlation and constant variance. this allows $\bf V$ to be a simple matrix where the diagonals are all $\sigma^2$ and every other number in the matrix is 0. Thus, $\bf V$ drops out and we are left with equation~\eqref{eq:olsmatrix},
\[
{\bf b}={\bf \left(X'X\right)^{-1}X'y}
\]
 The OLS estimator.

\section{Interpretation}
The general regression model is expressed algebracially like this
\begin{equation}
\hat{y}_i=\beta_0+\sum_p\beta_px_{ip}
\end{equation}
where $\sum_p\beta_px_{ip}$ is my shorthand for each $p$ predictor and its associated slope. We interpret the intercept, $\beta_0$, as the predicted value of $y$ when all predictors are equal to 0. Later we will do many neat things to control what the intercept means substantively by messing with what $x_p$ means.

Each slope for any predictor is then interpreted as the change in the predicted value of $y$, in the units of $y$ if we increase by one unit. Again, we can control what means substantively, and we will go over that later.


\section{Standardized coefficients}
One question that is often important to researchers is whether one variable is more important than another in predicting the outcome.

We can do this with the standardized regression coefficient. This is a scaled slope that takes into account the ratio of the standard deviation of the predictor to the standard deviation of the outcome
\begin{equation}
\beta_p^*=\beta_p\frac{s_p}{s_y}
\end{equation}
This procedure makes comparisons between predictors possible because its interpretation is changed from the change in $y$ for a one unit increase in $x$ (in the units of $x$) to the {\it standard deviation unit} change in $y$ for a {\it standard deviation increase} in $x$.

Be careful in doing this with dichotomous predictors.

\subsection{Standardized coefficient example}
Table~\ref{tab:wagesum} details the summary statistics for 200 observations from a dataset that collections wage, education, age and gender.

\begin{table}[htbp]
\caption{\label{tab:wagesum} Summary Statistics for Wage Model}\centering\medskip
\begin{tabular}{lcccc}
 & mean & sd & min & max \\ \hline
wage & 16.03 & 8.32 & 3.13 & 45.36 \\
edu & 13.48 & 2.91 & 5 & 20 \\
age & 36.73 & 12.31 & 16 & 64 \\ \hline
 \end{tabular}
\end{table}
\begin{table}[htbp]\centering
 \caption{Model predicting wages by education and age
\label{tab:wagereg}}
\begin{tabular}{lc}
\hline
Coefficients      &    Model  \\
\hline
edu     &    0.882***\\
      &   (0.184)  \\
age     &    0.218***\\
      &   (0.043)  \\
Intercept    &   -3.866  \\
      &   (3.061)  \\
\hline
\multicolumn{1}{l}{Model Statistics} \\
\hline
$N$      &   200.000  \\
$F$      &   22.947  \\
$R^2$     &    0.189  \\
$df$ Regression     &    2.000  \\
Sum of Squares Regression     &  2603.472  \\
$df$ Error    &   197.000 \\
Sum of Squares Error    &  11175.277  \\
\hline
\multicolumn{1}{l}{$SE$s in parentheses, $***p<0.001$} \\
\hline
\end{tabular}
\end{table}
Table~\ref{tab:wagereg} presents the regression model for wages as a function of education (edu) and age.
We can calculate the standardized coefficients using the information in Tables~\ref{tab:wagesum} and~\ref{tab:wagereg}. For example, the standardized coefficient for edu is
\[
\beta_{edu}^*=\beta_{edu}\frac{s_{edu}}{s_{wage}}=0.882\frac{2.91}{8.32}=0.308
\]
and for age is
\[
\beta_{age}^*=\beta_{age}\frac{s_{age}}{s_{wage}}=0.218\frac{12.31}{8.32}=0.322
\]
You will notice that even though the coefficient for education was larger, the standardized coefficient for age was larger because age had a larger standard deviation.
\section{Introduction to coding and interpreting nominal predictors}
So far we have only worked with regression predictors that are continuous in nature. However, in social science we often have nominal variables of interest that we think would make good predictors of continuous outcomes (for nominal outcomes, see the logit and probit chapter). This is just a brief introduction, we will have a chapter devoted to other things you can do with nominal variables.
\subsection{Two groups}
We can consider a variable dichotomous when it takes on only two values, 0 and 1. This can indicate a trait of some sort, or may simply be a way to organize a variable with two values.

In the bivariate case, the slope, standard error, and $t$-test of some outcome $y$ regressed on a dichotomous predictor $d$ will give the same answer as the independent groups' $t$-test. That is to say that in the model
\begin{equation}
y_i=\beta_0+\beta_1d_i+e_i
\end{equation}
the value of $\beta_1$ equals the difference between the average of $y$ when $d=1$ and $d=0$
\begin{equation}
\beta_1 = \left(\bar{y}|d=1\right)-\left(\bar{y}|d=0\right)
\end{equation}
Also, the standard error of $\beta_1$ is the same standard error from the independent test. As a result, the $t$-test and result are also the same.
\subsection{More than two groups}

We could use ANOVA to test generally whether these means are different. Table~\ref{tab:wordanova} told us with an $F$ test of 16.87, and with (2,2588) degrees of freedom, that groups were different.  Unfortunately, this doesn't tell us about who is different, just that at least one group is.

What we need is to enter dichotomous (or "dummy") variables into a regression model. This works by creating for each category in $x$ (except one reference group) new variables, $d_j$ where $j\in\left\{1 \ldots k\right\}$ that is equal to 1 if that case is a member of that group, and 0 otherwise:
\begin{equation}
d_j = \left\{ \begin{array}{ll}
     1 & \mbox{if $x = j$};\\
     0 & \mbox{otherwise}.\end{array} \right.
\end{equation}
Picking moderate as our reference group, and creating variables $lib$ and $con$, we can fit the following model
\[
\widehat{words}_i=\beta_0+\beta_1lib_i+\beta_2con_i
\]
This model is presented in Table~\ref{tab:wordmodel}. Note that the $F$-test is the same as what was reported by the ANOVA model.
\begin{table}[htbp]\centering
 \caption{Model predicting words correct by political affiliation
\label{tab:wordmodel}}
\begin{tabular}{lc}
\hline
Coefficients      &    Model  \\ \hline
lib &    0.513***\\
      &   (0.097)  \\
con &    0.422***\\
      &   (0.093)  \\
Intercept    &    5.812***\\
      &   (0.064)  \\
\hline
\multicolumn{1}{l}{Model Statistics} \\
\hline
$N$          &   2591.000  \\
$F$          &   16.869  \\
$R^2$        &    0.013  \\
$df$ Regression      &    2.000  \\
Sum of Squares Regression     &  134.432  \\
$df$ Error    &   2588.000  \\
Sum of Squares Error    &  10311.878  \\
\hline
\multicolumn{1}{l}{$SE$s in parentheses, $***p<0.001$} \\
\hline
\end{tabular}
\end{table}
A few things to note in this model. First, the intercept is the predicted value of $y$ when all predictors are equal to 0. Thus, this is the predicted value for the reference group (moderate).

Compare the constant with the mean of the moderate group. It's the same. Now, also compare the effect of $lib$, it is the difference in the means of the liberal group and the moderates. The effect of $con$ is the difference in the means of the conservative group and the moderates.

Sometimes researchers do not want to know the difference between a particular group and a reference group. Perhaps the research question is about the difference between a particular group and the population average, or specifically the average of group averages. In this case we use deviance coding. In deviance coding, instead of just a simple "1 for the group and 0 otherwise," we instead code each group as 1 for the group, -1 for the reference ($m$), and 0 otherwise:
\begin{equation}
d_j = \left\{ \begin{array}{ll}
     1 & \mbox{if $x = j$};\\
     -1 & \mbox{if $x = m$};\\
     0 & \mbox{otherwise}.\end{array} \right.
\end{equation}
we then fit the model with this coding
\begin{table}[htbp]\centering
 \caption{Model predicting words correct by political affiliation--deviance coding
\label{tab:wordreg_dc}}
\begin{tabular}{lc}
\hline
Coefficients      &    Model  \\ \hline
lib     &    0.201***\\
      &   (0.058)  \\
con     &    0.111* \\
      &   (0.055)  \\
Intercept    &    6.124***\\
      &   (0.040)  \\
\hline
\multicolumn{1}{l}{Model Statistics} \\
\hline
$N$          &   2591.000  \\
$F$          &   16.869  \\
$R^2$        &    0.013  \\
$df$ Regression      &    2.000  \\
Sum of Squares Regression     &  134.432  \\
$df$ Error    &   2588.000  \\
Sum of Squares Error    &  10311.878  \\
\hline
\multicolumn{1}{l}{$SE$s in parentheses, $***p<0.001$} \\
\hline
\end{tabular}
\end{table}
Now the intercept is an estimate of the average of group averages $((6.32 + 5.81 + 6.23)/3) = 6.12$, and each slope is the difference between that group and the average of group averages. For example, we see a difference of 0.20 words between liberals and and the average of group averages. Note, also, that all the model statistics are the same.

\begin{table}[htbp]\centering
 \caption{Model predicting words correct by political affiliation--linear contrasts
\label{tab:wordreg_contr}}
\begin{tabular}{lc}
\hline
Coefficients      &    Model  \\ \hline
$constrast_1$     &   -0.312***\\
      &   (0.054)  \\
$constrast_2$     &    0.045  \\
      &   (0.050)  \\
Intercept    &    6.124***\\
      &   (0.040)  \\
\hline
\multicolumn{1}{l}{Model Statistics} \\
\hline
$N$          &   2591.000  \\
$F$          &   16.869  \\
$R^2$        &    0.013  \\
$df$ Regression      &    2.000  \\
Sum of Squares Regression     &  134.432  \\
$df$ Error    &   2588.000  \\
Sum of Squares Error    &  10311.878  \\
\hline
\multicolumn{1}{l}{$SE$s in parentheses, $***p<0.001$} \\
\hline
\end{tabular}
\end{table}


A third method of coding variables is linear contrasts. Suppose, after our investigation, that we wanted to test other hypotheses such as how two groups compared to another. Suppose we wanted to test the null hypotheses that moderates were less smart than the average of liberals and conservatives. Such a hypothesis would be
\[
H_0:\mu_{mod}=\frac{\mu_{lib}+\mu_{con}}{2}
\]
which can be rewritten as
\[
H_0:0=1\times\mu_{mod}-0.5\times\mu_{lib}-0.5\times\mu_{con}
\]
A second hypothesis would test specifically whether liberals could be smarter than conservatives, ignoring moderates.
\[
H_0:\mu_{lib}=\mu_{con}
\]
which can be rewritten as
\[
H_0:0=0\times\mu_{mod}+1\times\mu_{lib}-1\times\mu_{con}
\]
and all could be symbolized generally with contrast coefficients
\[
H_0:0=c_1\times\mu_{mod}+c_2\times\mu_{lib}+c_3\times\mu_{con}
\]
where $c_1$, $c_2$, and $c_3$, are the contrast coefficients. We create new variables for each set of contrast coefficients. For example, we create a new variable $contrast_1$ that is equal to 1 ($c_1$) for moderates, -0.5 for both liberals ($c_2$) and conservatives ($c_3$). We then create another variable, $contrast_2$ that is equal to 0 ($c_1$) for moderates, 1 for liberals ($c_2$), and \[
\widehat{words}_i=\beta_0+\beta_1contrast_1+\beta_2contrast_2
\]
The results of this model are in Table~\ref{tab:wordreg_contr}. The effect of the first contrast shows that the moderate average is less than the average of liberals and conservatives. We see that the first contrast is statistically signifiant. However, the second contrast is not: liberals are not smarter than conservatives. Again, the model fit statistics are the same. For any set of contrasts, there are three sets of rules:
\begin{enumerate}
 \item The coefficients must add to 0 for each contrast
 \item The sum of the products of all contrasts must equal 0
 \item You must have $m$-1 contrasts, where $m$ is the number of categories
\end{enumerate}